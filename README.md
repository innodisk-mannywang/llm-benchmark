# llm-benchmark
### ğŸš€ Async & OpenAI-Compatible LLM Benchmark
A high-performance benchmarking tool for Large Language Models supporting async execution and OpenAI API format.
Easily measure latency, throughput, and token speed with concurrent requests and streaming response support. Perfect for evaluating both cloud and self-hosted models.

## âœ¨ Features
- **Real-time Resource Monitoring**: CPU, Memory, and GPU usage tracking
- **Enhanced Reporting**: CV-style console output with detailed metrics
- **Concurrent Testing**: Support for multiple concurrent connections
- **OpenAI Compatible**: Works with any OpenAI-compatible API endpoint
- **Docker Support**: Easy deployment with GPU acceleration support

## ğŸ‹ï¸ Pre-require
* Docker
    * [Docker 20.10+](https://docs.docker.com/engine/install/ubuntu/)
    * Setting docker to group
        ```bash
        sudo usermod -aG docker $USER
        ```
* NVIDIA GPU Support (Optional)
    * [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
    * Required for GPU resource monitoring
* Repository
    * clone the repository
      ```bash
      git clone https://github.com/TsoTing-Li/llm-benchmark.git
      ```
## ğŸ› ï¸ Build environment in docker
```bash
docker build -f docker/Dockerfile -t llm-benchmark:latest .
```

## ğŸ Startup
```bash
./run-vllm-gptoss.sh --no-pull
docker run -it --rm --name llm-benchmark-tool --gpus all --network=host -v $(pwd):/workspace llm-benchmark bash
```

## âœ¨ Example
* chat example (CV é¢¨æ ¼å ±å‘Š)
    ```bash
    python3 src/benchmark.py \
        --base_url http://localhost:8000 \
        --endpoint /v1/chat/completions \
        --model openai/gpt-oss-20b \
        --num_request 20 \
        --concurrency 8 \
        --prompt "how are you?" \
        --max_tokens 32 \
        --output_file report_llm_chat.json \
        --cv_style_output
    ```
* generate example (CV é¢¨æ ¼å ±å‘Š)
    ```bash
    python3 src/benchmark.py \
        --base_url http://localhost:8000 \
        --endpoint /v1/completions \
        --model openai/gpt-oss-20b \
        --num_request 100 \
        --concurrency 16 \
        --dataset_path ShareGPT_tiny.json \
        --max_tokens 32 \
        --output_file report_llm_dataset.json \
        --cv_style_output
    ```
- **For more parameter details, please check** [params.md](docs/params.md)

## ğŸ“Š Report

### Console Output (CV Style)
```
================================================================================
ğŸ” å¢å¼·ç‰ˆ LLM Benchmark å ±å‘Š
================================================================================

ğŸ“Š æ¸¬è©¦é…ç½®:
  â€¢ æ¨¡å‹: openai/gpt-oss-20b
  â€¢ è³‡æ–™é›†/æç¤º: ShareGPT_tiny.json
  â€¢ ç¸½è«‹æ±‚æ•¸: 100
  â€¢ ä½µç™¼é€£ç·šæ•¸: 16
  â€¢ åŸ·è¡Œæ™‚é–“: 3 ç§’
  â€¢ Provider: Innodisk IPA Dept.

âš¡ æ€§èƒ½æŒ‡æ¨™:
  â€¢ ç¸½è«‹æ±‚é€Ÿç‡: 18.51 req/s (requests per second)
  â€¢ æ¯é€£ç·šé€Ÿç‡: 1.16 req/s (requests per second per connection)
  â€¢ å¹³å‡å»¶é²: 826.15 ms (milliseconds)
  â€¢ å»¶é²ç¯„åœ: 340.67 - 1863.17 ms (milliseconds)
  â€¢ ç¸½ååé‡: 18.51 req/s (requests per second)
  â€¢ å¹³å‡TTFT: 349.34 ms (Time To First Token, milliseconds)
  â€¢ TTFTç¯„åœ: 21.00 - 1401.59 ms (milliseconds)

ğŸ’» è³‡æºä½¿ç”¨æƒ…æ³:
  â€¢ CPUä½¿ç”¨ç‡: 4.7% (æœ€é«˜: 7.2%) (percentage)
  â€¢ è¨˜æ†¶é«”ä½¿ç”¨ç‡: 3.7% (æœ€é«˜: 3.8%) (percentage)
  â€¢ GPUä½¿ç”¨ç‡: 28.7% (æœ€é«˜: 45.0%) (percentage)

ğŸ¯ è¼¸å‡ºçµ±è¨ˆ:
  â€¢ å¹³å‡å›æ‡‰tokens: 36.00 (tokens per request)
  â€¢ ç¸½è«‹æ±‚æ•¸: 100 (total requests)
  â€¢ æˆåŠŸè«‹æ±‚æ•¸: 100 (successful requests)

================================================================================
```

### JSON Report
```json
{
  "timestamp": "2025-09-05T01:34:24.349284",
  "configuration": {
    "model": "openai/gpt-oss-20b",
    "dataset": "ShareGPT_tiny.json",
    "concurrency": 16,
    "seconds": 3,
    "provider": "Innodisk IPA Dept."
  },
  "summary": {
    "total_concurrency": 16,
    "total_requests": 100,
    "total_runtime": 3.2857659539949964
  },
  "performance_metrics": {
    "fps": {
      "average": 1.9021440015838442,
      "min": 1.9021440015838442,
      "max": 1.9021440015838442
    },
    "latency_ms": {
      "average": 484.0487191599095,
      "min": 345.65436000411864,
      "max": 510.41040000563953
    },
    "throughput": {
      "average": 1.9021440015838442,
      "total": 30.434304025341508
    }
  },
  "resource_usage": {
    "cpu_percent": {"average": 4.7, "max": 7.2},
    "memory_percent": {"average": 3.7, "max": 3.8},
    "gpu_percent": {"average": 28.7, "max": 45.0}
  },
  "ttft_ms": {
    "average": 349.34,
    "min": 21.00,
    "max": 1401.59
  }
}
```
- **For more report detail, please check** [report.md](docs/report.md)

## ğŸ“ Units & Metrics

### Performance Metrics
- **req/s**: Requests per second (æ¯ç§’è«‹æ±‚æ•¸)
- **ms**: Milliseconds (æ¯«ç§’)
- **TTFT**: Time To First Token (é¦–Tokenå»¶é²æ™‚é–“)
- **tok/s**: Tokens per second (æ¯ç§’Tokenæ•¸)

### Resource Usage
- **%**: Percentage (ç™¾åˆ†æ¯”)
- **CPUä½¿ç”¨ç‡**: CPU utilization percentage
- **è¨˜æ†¶é«”ä½¿ç”¨ç‡**: Memory utilization percentage  
- **GPUä½¿ç”¨ç‡**: GPU utilization percentage

### Output Statistics
- **tokens**: Number of tokens generated per request
- **requests**: Total number of requests sent
- **successful requests**: Number of successfully completed requests